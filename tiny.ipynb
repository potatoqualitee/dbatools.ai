{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# When Data Processing Needs Knowledge\n",
        "\n",
        "## The Problem: When Regex Fails\n",
        "\n",
        "How do you programmatically know these headlines are about the same area?\n",
        "\n",
        "- \"Pacific Palisades evacuated due to fires\"  \n",
        "- \"Los Angeles under emergency status\"\n",
        "\n",
        "**Traditional approach fails:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Traditional regex approach - doesn't work!\n",
        "$headline1 = \"Pacific Palisades evacuated due to fires\"\n",
        "$headline2 = \"Los Angeles under emergency status\"\n",
        "\n",
        "# This fails because there's no pattern that connects them\n",
        "$headline1 -match \"\\bLA\\b\"\n",
        "$headline1 -match \"Los Angeles\"\n",
        "$headline1 -match \"\\bAngeles\\b\"\n",
        "$headline2 -match \"Pacific Palisades\"\n",
        "$headlineb -match \"90210\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Magic: Local LLMs with Structured Output\n",
        "\n",
        "Let's ask our local model a simple true/false question using **structured output**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup - make sure Ollama is running with: ollama serve\n",
        "# and you've pulled the model: ollama pull llama3.1\n",
        "\n",
        "# Demo 1: Simple true/false question\n",
        "$question = \"Is Pacific Palisades part of Los Angeles?\"\n",
        "\n",
        "# Define schema for true/false response\n",
        "$schema = @{\n",
        "    type = \"object\"\n",
        "    properties = @{\n",
        "        answer = @{\n",
        "            type = \"boolean\"\n",
        "        }\n",
        "    }\n",
        "    required = @(\"answer\")\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now make the REST API call (Yes! AI tools are just a bunch of JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make the API call\n",
        "$body = @{\n",
        "    model = \"llama3.1\"\n",
        "    messages = @(\n",
        "        @{\n",
        "            role = \"user\"\n",
        "            content = $question\n",
        "        }\n",
        "    )\n",
        "    stream = $false\n",
        "    format = $schema\n",
        "} | ConvertTo-Json -Depth 3\n",
        "\n",
        "$response = Invoke-RestMethod -Uri \"http://localhost:11434/api/chat\" -Method Post -Body $body\n",
        "\n",
        "# Parse the structured response\n",
        "$answerData = $response.message.content | ConvertFrom-Json\n",
        "\n",
        "\"Question: $question\"\n",
        "\"Answer: $($answerData.answer)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clean, structured data we can work with üéâ\n",
        "\n",
        "### Real-World Example: MP3 Filename Cleaning\n",
        "\n",
        "Let's see structured output tackle a practical problem - cleaning messy MP3 filenames using **world knowledge**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample messy MP3 filenames\n",
        "$messyMp3Files = @(\n",
        "    \"01_bohrhap_queen.mp3\",\n",
        "    \"material_girl-madonna85.mp3\",\n",
        "    \"hotel_cali_eagles1976.mp3\",\n",
        "    \"IMAGINE-J-LENNON-track2.mp3\",\n",
        "    \"hey_jude_(beetles)_1968_.mp3\",\n",
        "    \"billiejean_MJ_thriller.mp3\",\n",
        "    \"sweet_child_of_mine_gnr87.mp3\",\n",
        "    \"shake_it_off-taylorswift.mp3\",\n",
        "    \"purple-haze-jimmy_hendrix_1967.mp3\",\n",
        "    \"bohemian(queen)rhaps.mp3\",\n",
        "    \"smells_like_teen_spirit_nirvana91.mp3\",\n",
        "    \"halo_beyonce_2008.mp3\"\n",
        ")\n",
        "\n",
        "# Define the schema for clean artist/song extraction\n",
        "$schema = @{\n",
        "    type = \"object\"\n",
        "    properties = @{\n",
        "        artist = @{ type = \"string\" }\n",
        "        song = @{ type = \"string\" }\n",
        "    }\n",
        "    required = @(\"artist\", \"song\")\n",
        "}\n",
        "\n",
        "# The prompt that makes all the difference - includes examples!\n",
        "$prompt = @\"\n",
        "You are an AI that extracts artist and song names from messy MP3 filenames.\n",
        "\n",
        "Examples:\n",
        "1. \"hotel_cali_eagles1976.mp3\" ‚Üí {\"artist\": \"Eagles\", \"song\": \"Hotel California\"}\n",
        "2. \"rolling_in_the_deep-adele_2011.mp3\" ‚Üí {\"artist\": \"Adele\", \"song\": \"Rolling in the Deep\"}\n",
        "3. \"californication-RHCP.mp3\" ‚Üí {\"artist\": \"Red Hot Chili Peppers\", \"song\": \"Californication\"}\n",
        "\n",
        "Now, extract from this filename:\n",
        "\"@"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üßô‚Äç‚ôÇÔ∏è Processing files with AI magic: demonstrating 'asking tiny questions' - one file at a time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process each file individually (\"asking tiny questions\")\n",
        "$results = @()\n",
        "\n",
        "foreach ($file in $messyMp3Files) {\n",
        "    # Create the message for LLM\n",
        "    $msg = \"$prompt $file\"\n",
        "\n",
        "    # Create the payload with the schema object\n",
        "    $payload = @{\n",
        "        model = \"llama3.1\"\n",
        "        messages = @(@{role=\"user\"; content=$msg})\n",
        "        stream = $false\n",
        "        format = $schema\n",
        "    } | ConvertTo-Json -Depth 6 -Compress\n",
        "\n",
        "    # Call the local LLM API\n",
        "    $response = Invoke-RestMethod -Uri \"http://localhost:11434/api/chat\" -Method Post -Body $payload\n",
        "    $info = $response.message.content | ConvertFrom-Json\n",
        "\n",
        "    # Store result as PowerShell object\n",
        "    [pscustomobject]@{\n",
        "    Original = $file\n",
        "    Artist   = $info.artist\n",
        "    Song     = $info.song\n",
        "    Cleaned  = \"$($info.artist) - $($info.song).mp3\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Reality Check\n",
        "\n",
        "**Hardware matters A LOT with local models:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why This Matters for PowerShell Automation\n",
        "\n",
        "## üÜö Traditional vs AI-Enhanced Data Processing\n",
        "\n",
        "| Challenge                | Traditional Regex Approach | Local LLM Approach                 |\n",
        "| ------------------------ | -------------------------- | ---------------------------------- |\n",
        "| Geographic relationships | ‚ùå Impossible               | ‚úÖ Knows Pacific Palisades ‚àà LA     |\n",
        "| Misspelled data          | ‚ùå Brittle rules            | ‚úÖ Fixes \"beetles\" ‚Üí \"Beatles\"      |\n",
        "| Abbreviations            | ‚ùå Endless patterns         | ‚úÖ Expands \"MJ\" ‚Üí \"Michael Jackson\" |\n",
        "| Context understanding    | ‚ùå No context               | ‚úÖ Understands intent               |\n",
        "| Domain knowledge         | ‚ùå Requires databases       | ‚úÖ Built-in world knowledge         |\n",
        "\n",
        "## üî• Benefits of Local LLMs\n",
        "\n",
        "* üåê No internet required\n",
        "* üí∞ No API costs\n",
        "* üîí Complete privacy\n",
        "* üéØ Perfect for batch processing\n",
        "* üß† Built-in world knowledge\n",
        "* üìä Returns clean PowerShell objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Recommendations\n",
        "\n",
        "**Popular local models to try:**\n",
        "\n",
        "\n",
        "| Model           | Parameters | Description |\n",
        "|---------------|------------|-------------|\n",
        "| **LLaMA (Meta)** | 7B, 13B, 70B | My favorite. The 70B variant achieves excellent factual accuracy but needs serious hardware. The 7B/13B versions are more accessible but may hallucinate more. Solid instruction-following capabilities. |\n",
        "| **Mistral 7B** | 7B | Highly efficient, outperforming many larger models. Notable for lower hallucination rates, making it more reliable for factual queries. |\n",
        "| **Gemma (Google)** | 2B, 7B | 2B variant designed for CPU/mobile use. The instruction-tuned 7B model competes well with other 7B models. Known for a friendly style and structured output using bullet points and Markdown. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaways\n",
        "\n",
        "**üéØ When to use Local LLMs for PowerShell automation:**\n",
        "\n",
        "1. **Beyond Regex**: When you need world knowledge, not patterns\n",
        "2. **Privacy Matters**: Sensitive data stays local\n",
        "3. **Batch Processing**: Perfect for overnight jobs\n",
        "4. **Cost Control**: No per-request API fees\n",
        "5. **Offline Capable**: No internet dependency\n",
        "6. **PowerShell Objects**: Clean, pipeable data\n",
        "\n",
        "**üí° Remember: Ask tiny questions, get reliable answers!**\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try this notebook with your own messy data\n",
        "- Experiment with different models in Ollama\n",
        "- Check out the full blog series: **AI Integration for Automation Engineers**\n",
        "- Consider local LLMs for your next PowerShell automation project!\n",
        "\n",
        "**Questions? Let's chat after the session!** üé§"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PowerShell",
      "language": "powershell",
      "name": "powershell"
    },
    "language_info": {
      "codemirror_mode": "shell",
      "file_extension": ".ps1",
      "mimetype": "text/x-sh",
      "name": "powershell"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
